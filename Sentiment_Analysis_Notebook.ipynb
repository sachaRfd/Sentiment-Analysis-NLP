{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachaRfd/Sentiment-Analysis-NLP/blob/main/Sentiment_Analysis_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Sentiment Analysis using IMDB PyTorch Dataset and simple LSTM:"
      ],
      "metadata": {
        "id": "bvm1VVTO4xsl"
      },
      "id": "bvm1VVTO4xsl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Imports:"
      ],
      "metadata": {
        "id": "6zk3XNbt4_kI"
      },
      "id": "6zk3XNbt4_kI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata  # Install Torch Datasets\n",
        "!pip install nltk  # Import the Natural Language Toolkit --> Most Common\n",
        "\n",
        "import nltk  # Download key files\n",
        "nltk.download('punkt')  # Sequence Tokeniser\n",
        "nltk.download('stopwords')  # List of Most Common StopWords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string \n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import pad\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "\n",
        "# Set Device to GPU is available - otherwise set to CPU: \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Your Current Device is {device}')  # Check the Colab Device we are using\n",
        "\n",
        "from torchtext import data, datasets  # Import the datasets\n",
        "from sklearn.model_selection import train_test_split  # Import splitting function\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torchdata\n",
        "\n",
        "from torchtext.vocab import GloVe  # Import the Glove Embedding"
      ],
      "metadata": {
        "id": "IdJSwr7e4-4-",
        "outputId": "1a36714b-3e32-4009-e06c-c27db2ea4cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IdJSwr7e4-4-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 72.1 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 1.1 MB/s eta 0:00:48tcmalloc: large alloc 1147494400 bytes == 0x3aa3c000 @  0x7f5425f61615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 887.4 MB 1.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 11 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 61.1 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata) (0.38.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, urllib3, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.6.0 torch-1.13.1 torchdata-0.5.1 urllib3-1.25.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Current Device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "lRb2CRJQeMWs"
      },
      "id": "lRb2CRJQeMWs",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the Train, Validation and Training Sets ready: "
      ],
      "metadata": {
        "id": "3StZWGLJ64ht"
      },
      "id": "3StZWGLJ64ht"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the train and test splits form the IMDB Dataset\n",
        "train_dataset, test_dataset  = datasets.IMDB(root = '.data', split = ('train', 'test'))\n",
        "\n",
        "# Let's now split the test set into a test and validation set: \n",
        "test_dataset, valid_dataset = train_test_split(list(test_dataset), train_size=.8)\n"
      ],
      "metadata": {
        "id": "J_uRE7P26-JJ"
      },
      "id": "J_uRE7P26-JJ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset: \n",
        "IMDB Reviews\n"
      ],
      "metadata": {
        "id": "fXfE-9e56_QH"
      },
      "id": "fXfE-9e56_QH"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of the Train set is {len(list(train_dataset))}')  #  Have to Convert to List\n",
        "print(f'The shape of the Validation set is {len(valid_dataset)}')\n",
        "print(f'The shape of the Test set is {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "ObXgtrWG7Yrd",
        "outputId": "45e0374e-5abe-451a-dd7a-69553978ff96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ObXgtrWG7Yrd",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the Train set is 25000\n",
            "The shape of the Validation set is 5000\n",
            "The shape of the Test set is 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of the training Dataset is 25_000. \n",
        "\n",
        "Let's Check if our data is balanced in the training set: "
      ],
      "metadata": {
        "id": "PwZZaanj7sKH"
      },
      "id": "PwZZaanj7sKH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to check for balanced dataset\n",
        "dataframe_ = pd.DataFrame(list(train_dataset), columns=['Y', 'x'])\n",
        "dataframe_.Y.value_counts()"
      ],
      "metadata": {
        "id": "OSQ0AsiP7vO0",
        "outputId": "995ae923-a354-4b04-bb91-816cc6dc7306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OSQ0AsiP7vO0",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    12500\n",
              "2    12500\n",
              "Name: Y, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's now visualise some of the reviews: "
      ],
      "metadata": {
        "id": "0I7jEXXh7Vvk"
      },
      "id": "0I7jEXXh7Vvk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the First 2 Reviews\n",
        "list(train_dataset)[:5]"
      ],
      "metadata": {
        "id": "0sTYA9Tg7ElX",
        "outputId": "48631b43-372a-45ef-b07a-932f8192c017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0sTYA9Tg7ElX",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,\n",
              "  'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'),\n",
              " (1,\n",
              "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'),\n",
              " (1,\n",
              "  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"),\n",
              " (1,\n",
              "  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\"),\n",
              " (1,\n",
              "  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To summarise the dataset: \n",
        "- The dataset consists of Movie reviews taken from IMDB\n",
        "- The train set is formed of 25_000 reviews\n",
        "- The validation set of 5_000 reviews\n",
        "- and the Test set is of 20_000 reviews. \n",
        "- In the Y variable, a 1 consists of a Negative Review and 2 a Positive Review\n",
        "- We can also see that our dataset is BALANCED, with 12_500 bad reviews and 12_5000 good reviews."
      ],
      "metadata": {
        "id": "fTsu999e7Khe"
      },
      "id": "fTsu999e7Khe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing: \n",
        "\n",
        "For simlpe NLP applications, the data has to be processed in a certain manner: \n",
        "- Things to Check For:\n",
        "  - All lower-case text\n",
        "  - No Numbers in text\n",
        "  - No Punctutation - Good for generalisation - eventhough some people use punctuation to show sentiment\n",
        "- Transformation of the sentences into list of tokens - Therefore the sentence becomes a list of words\n",
        "- We have to tokenise the words - We will be using words from the GloVe library.\n",
        "  - We want to get the Index of our words in the GloVe library.\n",
        "- Padding of the sentences is also required as some of the reviews are very long or relatively short. Let's use a maximum padding of 150 here - For no reason."
      ],
      "metadata": {
        "id": "1mV1sWUG-cLb"
      },
      "id": "1mV1sWUG-cLb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Functions: \n",
        "- To Remove Numbers in the sentences\n",
        "- To Remove Punctuation\n",
        "- To Tokenise the sentences\n",
        "- To Remove Unwanted Stopwords\n",
        "- To Get the Index of the words in the GloVe library\n",
        "- To Pad the Sentences\n",
        "- A final Function which transforms the inputted test by using all the above functions and converting the sentences to lowercase."
      ],
      "metadata": {
        "id": "zUs4TgkkBQZM"
      },
      "id": "zUs4TgkkBQZM"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  '''Function to Remove Numbers from inputted text'''\n",
        "  \n",
        "  text = ''.join(word for word in text if not word.isdigit())\n",
        "  return text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  '''Function to Remove all Punctuation from inputted text'''\n",
        "\n",
        "  for punctuation in string.punctuation:\n",
        "     text = text.replace(punctuation, '')  # Replace the Punctuation with empty space\n",
        "  return text\n",
        "\n",
        "def tokenize(text):\n",
        "  '''Function to Tokenise any inputted text using NLTK tokenise'''\n",
        "\n",
        "  word_tokens = word_tokenize(text)  # Tokenise Using the NLTK Tokenise Function\n",
        "  return word_tokens\n",
        "\n",
        "def remove_stopwords(word_tokens, language='english'):\n",
        "  '''Function to remove all stopwords in given language from the inputted words tokens'''\n",
        "\n",
        "  stop_words = set(stopwords.words(language))  # Most common English Stopwords\n",
        "  word_tokens = [w for w in word_tokens if not w in stop_words]  # Get list of words if they are not stopwords\n",
        "  return word_tokens\n",
        "\n",
        "glove = GloVe(dim='50', name='6B', max_vectors=20000)  # Get the Glove with 50 dimension vector with a vocabulary size of 20_000\n",
        "\n",
        "def get_index(text, vocab=glove):\n",
        "  '''Function that gets the index of each token in a text from the GloVe Library'''\n",
        "\n",
        "  embedded_text = []\n",
        "  for word in text:\n",
        "     try:\n",
        "         embedded_text.append(glove.stoi[word])  # Get String to Integer\n",
        "     except:\n",
        "         pass\n",
        "  return embedded_text  # return list of the indices of the tokenised words in the GloVe library\n",
        "\n",
        "\n",
        "def pad_sentence(text, MAX_LENGTH = 100):\n",
        "  ''' Function that Pads a sentence to a given length'''\n",
        "\n",
        "  if text.shape[0]>=MAX_LENGTH:\n",
        "      return text[:MAX_LENGTH]\n",
        "  else:\n",
        "      return pad(text, (0, MAX_LENGTH-text.shape[0]), 'constant',0).long()\n",
        "\n",
        "\n",
        "# Final Transform Function: \n",
        "\n",
        "def transform_text(text):\n",
        "  '''Function that applies all the Data-Preprocessing Functions'''\n",
        "  \n",
        "  text = text.lower()\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = tokenize(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = torch.tensor(get_index(text)).long()\n",
        "  return pad_sentence(text)"
      ],
      "metadata": {
        "id": "NwfFDYvXBNsd",
        "outputId": "d02fb5cf-aa2f-4e66-f0ce-217b20f48bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NwfFDYvXBNsd",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
            "100%|█████████▉| 19999/20000 [00:00<00:00, 46678.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have setup our data-preprocessing, let's test it out on an example from our training dataset: "
      ],
      "metadata": {
        "id": "FBDR7GjDFC7_"
      },
      "id": "FBDR7GjDFC7_"
    },
    {
      "cell_type": "code",
      "source": [
        "example_train = list(train_dataset)[5][1]  # Get a random train data\n",
        "transform_text(example_train)"
      ],
      "metadata": {
        "id": "byexTK2OFJdI",
        "outputId": "4b7d3288-a796-4228-8040-f7637c0c01e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "byexTK2OFJdI",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   54,   339,   220,   674,  1588,  2891,  8560,  1588,   978,  1607,\n",
              "          921,  2001, 12073,   117,  2837,   219,  1739, 11184, 10487,   122,\n",
              "          151, 12425,   175,  2782,  1378,  6959,   152,   164,  9797,  4629,\n",
              "          364,   319,  1607,  1380,   570,  6801,  5412,   521,   298,  3468,\n",
              "         1254,   492,  1797,  7582,   151,  1507,   978,    69,   580,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our pre-processing function seem to work as we want - Let's finalise the dataset with a dataloader. \n",
        "\n",
        "To not apply our transform function to all the data at once, let'd just apply it batch by batch from the dataloader using transform_batch function: "
      ],
      "metadata": {
        "id": "RC8smoypHgYp"
      },
      "id": "RC8smoypHgYp"
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = torch.tensor([item[0] for item in list(train_dataset)])-1\n",
        "train_x = torch.stack([transform_text(item[1]) for item in list(train_dataset)])\n",
        "\n",
        "val_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n",
        "val_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])\n",
        "\n",
        "test_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n",
        "test_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])"
      ],
      "metadata": {
        "id": "GG2QGUwO6-iy"
      },
      "id": "GG2QGUwO6-iy",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    \n",
        "    X_embedded = torch.stack([transform_text(txt) for txt in X])  # Get the transformed - embedded text\n",
        "    \n",
        "    return X_embedded, torch.tensor(Y).long()-1  # Return the Embedded text and the Y variable as .long() as it is a categorical label\n",
        "\n",
        "train_dataset=  to_map_style_dataset(train_dataset)  # We will be using the to_map_style_dataset as it CHECK WHAT IT DOES\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=transform_batch, shuffle=True)  # Make sure to have shuffle on true for best training"
      ],
      "metadata": {
        "id": "4o6jcFJdHfar"
      },
      "id": "4o6jcFJdHfar",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the batch sizes\n",
        "# for X, Y in train_loader:\n",
        "#     print(X.shape, Y.shape)\n",
        "#     break"
      ],
      "metadata": {
        "id": "orQGoLED3nuW",
        "outputId": "f5c5e443-c6c4-43cd-d801-cdc76c811af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "orQGoLED3nuW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 150]) torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the GloVe vectors in our embedding layer, where we inputted 50 dimensions for each vector and a vocabulary size of 20_000.\n",
        "\n",
        "Here is an implementation of a embedded layer taken from my lecture in DL:"
      ],
      "metadata": {
        "id": "67mWN8dW4Pco"
      },
      "id": "67mWN8dW4Pco"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=True):\n",
        "    num_embeddings, embedding_dim = weights_matrix.size()\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim,padding_idx=0)\n",
        "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "metadata": {
        "id": "I9zCvBfS4s8a"
      },
      "id": "I9zCvBfS4s8a",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we have all of the preprocessing, the embedding, and the data loaders ready, we can start thinking of Deep Learning Models."
      ],
      "metadata": {
        "id": "-H7NZnwU4zXA"
      },
      "id": "-H7NZnwU4zXA"
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hid_dim, output_dim,drop_out=0., num_layers = 2):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(glove.vectors, False)\n",
        "        \n",
        "        n_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers,dropout=drop_out, batch_first=True)\n",
        "        self.linear = nn.Linear(hid_dim,100)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(100, output_dim)\n",
        "        self.dropout = nn.Dropout(.5)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        std= 1.0 / np.sqrt(self.hid_dim)\n",
        "        \n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "        \n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "\n",
        "        batch_size, seq_len,  _ = embedded.shape\n",
        "        hid_dim = self.lstm.hidden_size\n",
        "            \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        outputs = outputs[:, -1]\n",
        "        \n",
        "        prediction = self.fc(self.dropout(self.relu(self.linear(outputs))))\n",
        "\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "FMjZYO3L55X3"
      },
      "id": "FMjZYO3L55X3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CalcValLossAndAccuracy(model, loss_fn, val_X, val_Y):\n",
        "    \n",
        "    #print(f'Calculating Epoch Loss and Accuracy:')\n",
        "    \n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, Y, title = (val_x, val_y,'Validation')\n",
        "        X = val_X.to(device)\n",
        "        Y = val_Y.to(device)\n",
        "            \n",
        "        outputs = model(X).squeeze()\n",
        "        loss = loss_fn(outputs, Y.float())\n",
        "            \n",
        "        preds = [1 if p>=.5 else 0 for p in torch.sigmoid(outputs)]\n",
        "        accuracy = accuracy_score(Y.detach().cpu().numpy().tolist(),preds)\n",
        "            \n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss)\n",
        "\n",
        "        \n",
        "        print(f'{title} Loss : {loss:.3f}')\n",
        "        print(f\"{title} Accuracy  : {accuracy:.3f}\")\n",
        "    \n",
        "    return losses, accuracies\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, epochs=10):\n",
        "    train_losses = []\n",
        "    train_accuracy = []\n",
        "    val_losses = []\n",
        "    val_accuracy = []\n",
        "    \n",
        "    for i in range(1, epochs+1):\n",
        "        \n",
        "        print('-'*100)\n",
        "        print(f'EPOCH {i}')\n",
        "        print('-'*100)\n",
        "        \n",
        "        epoch_losses = []\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        for X, Y in tqdm(train_loader, colour='BLUE'):\n",
        "\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            \n",
        "            Y_preds = model(X).squeeze()\n",
        "            loss = loss_fn(Y_preds, Y.float())\n",
        "            \n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(epoch_losses).mean()))\n",
        "        \n",
        "        losses, acc = CalcValLossAndAccuracy(model, loss_fn, X, Y)\n",
        "        train_losses.append(losses[0])\n",
        "        train_accuracy.append(losses[0])\n",
        "        val_losses.append(acc[0])\n",
        "        val_accuracy.append(acc[0])\n",
        "        \n",
        "    return train_losses, val_losses, train_accuracy, val_accuracy\n",
        "\n",
        "def evaluateModel(model, loss_fn, x, Y):\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    Y = Y.to(device)\n",
        "\n",
        "    outputs = model(x).squeeze()\n",
        "\n",
        "    preds = [1 if p>=.5 else 0 for p in torch.sigmoid(outputs)]\n",
        "    accuracy = accuracy_score(Y.detach().cpu().numpy().tolist(),preds)\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "06I2T3Oh5_IP"
      },
      "id": "06I2T3Oh5_IP",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "text_classifier_2_epochs = LSTM(20,1).to(device)\n",
        "\n",
        "optimizer = RMSprop(text_classifier_2_epochs.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"MODEL ARCHITECTURE:\")\n",
        "print(text_classifier_2_epochs)\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "TrainModel(text_classifier_2_epochs, loss_fn, optimizer, train_loader, epochs)"
      ],
      "metadata": {
        "id": "zkHV-7w46EnV",
        "outputId": "9f13b4f9-e19c-41da-ef2d-528d696d7ae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zkHV-7w46EnV",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TRAINING\n",
            "MODEL ARCHITECTURE:\n",
            "LSTM(\n",
            "  (embedding): Embedding(20000, 50, padding_idx=0)\n",
            "  (lstm): LSTM(50, 20, num_layers=2, batch_first=True)\n",
            "  (linear): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            " \n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.695\n",
            "Validation Loss : 0.692\n",
            "Validation Accuracy  : 0.521\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.622\n",
            "Validation Loss : 0.490\n",
            "Validation Accuracy  : 0.804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor(0.6917, device='cuda:0'), tensor(0.4898, device='cuda:0')],\n",
              " [0.5212264150943396, 0.8042452830188679],\n",
              " [tensor(0.6917, device='cuda:0'), tensor(0.4898, device='cuda:0')],\n",
              " [0.5212264150943396, 0.8042452830188679])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Checkout the model we have trained: "
      ],
      "metadata": {
        "id": "CJ0yaYnY8K4G"
      },
      "id": "CJ0yaYnY8K4G"
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[10]"
      ],
      "metadata": {
        "id": "XAUJnboJ8Obb",
        "outputId": "191a6906-92f6-429e-b70c-8bfc70a0cfb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XAUJnboJ8Obb",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,\n",
              " \"The Man Who Knew Too Much{1956}is a remake of a film that Alfred Hitchcock made in England in 1934 with the same name. In my opinion, his later effort is far superior. Many critics and fans of Alfred Hitchcock will argue that the remake is mediocre and doesn't have the spine tingling suspense of the original with Peter Lorre. In both films the plot is essentially the same, except the original is set in Switzerland and the remake in Marrakech . It tells the story of a married couple {James Stewart and Doris Day}vacationing with their young son and meeting a suspicious man, that is very curious about their past. It just so happens, he's an agent that's looking for a couple involved in a plot to assassinate a world leader.Then he gets stabbed in a Marrakeck market because of it being found out that he's a spy,and proceeds to fall into Stewart's arms.Dying,he tells him the whole story of the assassination plot.Stewart and Day then find out that another couple they met were the couple the agent was looking for and have kidnapped their son.The film contains excellent performances by Stewart and Day,in a straight dramatic role,as worried and frightened parents.This film proved that Doris Day could act in suspenseful dramas as well as carefree musicals.The direction by Alfred Hitchcock is top-notch.The film keeps you on the edge of your seat every minute.The scene in Albert Hall is a classic.The original is so slow-paced and drab.I don't know how people can compare the two.Just watch the remake and you'll enjoy it.I give the movie a 9 out of 10.\")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_classifier_2_epochs.eval()\n",
        "with torch.no_grad():\n",
        "    print('Predicted values with only 2 epochs: ')\n",
        "    print(torch.sigmoid(text_classifier_2_epochs(test_x[:5].to(device))))\n",
        "    print(test_y[:5])\n",
        "    print(f'The Accuracy of our model with 2 epochs is of {evaluateModel(text_classifier_2_epochs, loss_fn, test_x, test_y)}')"
      ],
      "metadata": {
        "id": "artbl6s_8T--",
        "outputId": "dc1b148a-11f6-40eb-9e00-030967bc3e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "artbl6s_8T--",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values with only 2 epochs: \n",
            "tensor([[0.1600],\n",
            "        [0.3135],\n",
            "        [0.1773],\n",
            "        [0.7810],\n",
            "        [0.1773]], device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0])\n",
            "The Accuracy of our model with 2 epochs is of 0.76305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's now try training the model with 10 epochs:"
      ],
      "metadata": {
        "id": "8M-OwnCX_bCC"
      },
      "id": "8M-OwnCX_bCC"
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "text_classifier_10_epochs = LSTM(20,1).to(device)\n",
        "\n",
        "optimizer = RMSprop(text_classifier_10_epochs.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"MODEL ARCHITECTURE:\")\n",
        "print(text_classifier_10_epochs)\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "TrainModel(text_classifier_10_epochs, loss_fn, optimizer, train_loader, epochs)"
      ],
      "metadata": {
        "id": "433ZRjln-TZh",
        "outputId": "30254262-fbde-4470-b460-abd527777d53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "433ZRjln-TZh",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TRAINING\n",
            "MODEL ARCHITECTURE:\n",
            "LSTM(\n",
            "  (embedding): Embedding(20000, 50, padding_idx=0)\n",
            "  (lstm): LSTM(50, 20, num_layers=2, batch_first=True)\n",
            "  (linear): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            " \n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.695\n",
            "Validation Loss : 0.692\n",
            "Validation Accuracy  : 0.521\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.622\n",
            "Validation Loss : 0.490\n",
            "Validation Accuracy  : 0.804\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.458\n",
            "Validation Loss : 0.352\n",
            "Validation Accuracy  : 0.880\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:28<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.396\n",
            "Validation Loss : 0.335\n",
            "Validation Accuracy  : 0.889\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.334\n",
            "Validation Loss : 0.234\n",
            "Validation Accuracy  : 0.920\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.302\n",
            "Validation Loss : 0.234\n",
            "Validation Accuracy  : 0.925\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.263\n",
            "Validation Loss : 0.255\n",
            "Validation Accuracy  : 0.892\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:26<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.247\n",
            "Validation Loss : 0.208\n",
            "Validation Accuracy  : 0.939\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.224\n",
            "Validation Loss : 0.241\n",
            "Validation Accuracy  : 0.910\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:29<00:00,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.209\n",
            "Validation Loss : 0.118\n",
            "Validation Accuracy  : 0.974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor(0.6917, device='cuda:0'),\n",
              "  tensor(0.4898, device='cuda:0'),\n",
              "  tensor(0.3518, device='cuda:0'),\n",
              "  tensor(0.3346, device='cuda:0'),\n",
              "  tensor(0.2336, device='cuda:0'),\n",
              "  tensor(0.2339, device='cuda:0'),\n",
              "  tensor(0.2550, device='cuda:0'),\n",
              "  tensor(0.2081, device='cuda:0'),\n",
              "  tensor(0.2414, device='cuda:0'),\n",
              "  tensor(0.1183, device='cuda:0')],\n",
              " [0.5212264150943396,\n",
              "  0.8042452830188679,\n",
              "  0.8797169811320755,\n",
              "  0.8891509433962265,\n",
              "  0.9198113207547169,\n",
              "  0.9245283018867925,\n",
              "  0.8915094339622641,\n",
              "  0.9386792452830188,\n",
              "  0.910377358490566,\n",
              "  0.9740566037735849],\n",
              " [tensor(0.6917, device='cuda:0'),\n",
              "  tensor(0.4898, device='cuda:0'),\n",
              "  tensor(0.3518, device='cuda:0'),\n",
              "  tensor(0.3346, device='cuda:0'),\n",
              "  tensor(0.2336, device='cuda:0'),\n",
              "  tensor(0.2339, device='cuda:0'),\n",
              "  tensor(0.2550, device='cuda:0'),\n",
              "  tensor(0.2081, device='cuda:0'),\n",
              "  tensor(0.2414, device='cuda:0'),\n",
              "  tensor(0.1183, device='cuda:0')],\n",
              " [0.5212264150943396,\n",
              "  0.8042452830188679,\n",
              "  0.8797169811320755,\n",
              "  0.8891509433962265,\n",
              "  0.9198113207547169,\n",
              "  0.9245283018867925,\n",
              "  0.8915094339622641,\n",
              "  0.9386792452830188,\n",
              "  0.910377358490566,\n",
              "  0.9740566037735849])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model: "
      ],
      "metadata": {
        "id": "JQiJPp8M_Z-H"
      },
      "id": "JQiJPp8M_Z-H"
    },
    {
      "cell_type": "code",
      "source": [
        "text_classifier_10_epochs.eval()\n",
        "print(f'The Accuracy of our model with 10 epochs is of {evaluateModel(text_classifier_10_epochs, loss_fn, test_x, test_y)}')"
      ],
      "metadata": {
        "id": "LMcM-kLi_YjI",
        "outputId": "43478719-d7b6-4790-a92b-7e0734ccc648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LMcM-kLi_YjI",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of our model with 10 epochs is of 0.8148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's use the next cell for hyper-parameter tuning: "
      ],
      "metadata": {
        "id": "qF7xx0tYIQ4-"
      },
      "id": "qF7xx0tYIQ4-"
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "text_classifier_hyper = LSTM(20,1, 1).to(device)\n",
        "\n",
        "optimizer = RMSprop(text_classifier_hyper.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"MODEL ARCHITECTURE:\")\n",
        "print(text_classifier_hyper)\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "TrainModel(text_classifier_hyper, loss_fn, optimizer, train_loader, epochs)\n",
        "evaluateModel(text_classifier_hyper, loss_fn, test_x, test_y)"
      ],
      "metadata": {
        "id": "7vq206ufIc0U",
        "outputId": "3bd159bb-5df9-4202-d94c-264c00c149e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7vq206ufIc0U",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TRAINING\n",
            "MODEL ARCHITECTURE:\n",
            "LSTM(\n",
            "  (embedding): Embedding(20000, 50, padding_idx=0)\n",
            "  (lstm): LSTM(50, 20, batch_first=True)\n",
            "  (linear): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            " \n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 1\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:28<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.693\n",
            "Validation Loss : 0.691\n",
            "Validation Accuracy  : 0.498\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 2\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:26<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.664\n",
            "Validation Loss : 0.543\n",
            "Validation Accuracy  : 0.757\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 3\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:26<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.473\n",
            "Validation Loss : 0.381\n",
            "Validation Accuracy  : 0.868\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 4\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.393\n",
            "Validation Loss : 0.347\n",
            "Validation Accuracy  : 0.875\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 5\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:26<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.336\n",
            "Validation Loss : 0.215\n",
            "Validation Accuracy  : 0.941\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 6\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:26<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.317\n",
            "Validation Loss : 0.310\n",
            "Validation Accuracy  : 0.901\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 7\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:27<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.264\n",
            "Validation Loss : 0.218\n",
            "Validation Accuracy  : 0.939\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 8\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:35<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.253\n",
            "Validation Loss : 0.226\n",
            "Validation Accuracy  : 0.929\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 9\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:29<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.229\n",
            "Validation Loss : 0.237\n",
            "Validation Accuracy  : 0.915\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EPOCH 10\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 49/49 [00:29<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.205\n",
            "Validation Loss : 0.212\n",
            "Validation Accuracy  : 0.934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8215"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_classifier_hyper.eval()\n",
        "with torch.no_grad():\n",
        "    print('Predicted values with only 10 epochs: ')\n",
        "    print(torch.sigmoid(text_classifier_hyper(test_x[:10].to(device))))\n",
        "    print(test_y[:10])\n",
        "    print(f'The Accuracy of our model whith these hyper parameters is of {evaluateModel(text_classifier_hyper, loss_fn, test_x, test_y)}')"
      ],
      "metadata": {
        "id": "f1-4gIAEQmro",
        "outputId": "9e200a86-169e-4a8c-9eb8-1266ce32b3ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f1-4gIAEQmro",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values with only 10 epochs: \n",
            "tensor([[0.2351],\n",
            "        [0.0915],\n",
            "        [0.0650],\n",
            "        [0.9899],\n",
            "        [0.0670],\n",
            "        [0.0536],\n",
            "        [0.9788],\n",
            "        [0.9686],\n",
            "        [0.9752],\n",
            "        [0.4494]], device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1])\n",
            "The Accuracy of our model whith these hyper parameters is of 0.8215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Try using the same above model but with the Mish activation function - This one seemed to work very well with generative models as it is self-regularising. \n",
        "\n",
        "Let's see how it performs here:"
      ],
      "metadata": {
        "id": "NW7gWp8qiXdN"
      },
      "id": "NW7gWp8qiXdN"
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Mish(nn.Module):\n",
        "    def __init__(self, hid_dim, output_dim,drop_out=0., num_layers = 2):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(glove.vectors, False)\n",
        "        \n",
        "        n_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers,dropout=drop_out, batch_first=True)\n",
        "        self.linear = nn.Linear(hid_dim,100)\n",
        "        self.relu = nn.Mish()\n",
        "        self.fc = nn.Linear(100, output_dim)\n",
        "        self.dropout = nn.Dropout(.5)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        std= 1.0 / np.sqrt(self.hid_dim)\n",
        "        \n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "        \n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "\n",
        "        batch_size, seq_len,  _ = embedded.shape\n",
        "        hid_dim = self.lstm.hidden_size\n",
        "            \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        outputs = outputs[:, -1]\n",
        "        \n",
        "        prediction = self.fc(self.dropout(self.relu(self.linear(outputs))))\n",
        "\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "kpNGJ34zinQL"
      },
      "id": "kpNGJ34zinQL",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "text_classifier_Mish = LSTM_Mish(20,1, 1).to(device)\n",
        "\n",
        "optimizer = RMSprop(text_classifier_Mish.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"MODEL ARCHITECTURE:\")\n",
        "print(text_classifier_Mish)\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "TrainModel(text_classifier_Mish, loss_fn, optimizer, train_loader, epochs)\n",
        "evaluateModel(text_classifier_Mish, loss_fn, test_x, test_y)"
      ],
      "metadata": {
        "id": "xM7yhaWei2xe",
        "outputId": "bdc0f27d-0a99-4d30-c1f6-91e7241699be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "id": "xM7yhaWei2xe",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d6018f124a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtext_classifier_Mish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Mish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_classifier_Mish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-b8a837f7e984>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hid_dim, output_dim, drop_out, num_layers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLSTM_Mish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-0.53915 with 2 epochs and 20 hidden dim and 2 lstm layers\n",
        "\n",
        "-0.54175 with 2 epochs and 20 hidden dim and 3 lstm layers\n",
        "\n",
        "-0.5472 with 2 epochs and 20 hidden dim and 5 lstm layers\n",
        "\n",
        "-0.61775 with 2 epochs and 40 hidden dim and 5 lstm layers\n",
        "\n",
        "-0.50055 with 10 epochs and 40 hidden dim and 5 lstm layers"
      ],
      "metadata": {
        "id": "9qgOqrG-YNhY"
      },
      "id": "9qgOqrG-YNhY"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}