{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachaRfd/Sentiment-Analysis-NLP/blob/main/Sentiment_Analysis_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Sentiment Analysis using IMDB PyTorch Dataset and simple LSTM:"
      ],
      "metadata": {
        "id": "bvm1VVTO4xsl"
      },
      "id": "bvm1VVTO4xsl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Imports:"
      ],
      "metadata": {
        "id": "6zk3XNbt4_kI"
      },
      "id": "6zk3XNbt4_kI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata  # Install Torch Datasets\n",
        "!pip install nltk  # Import the Natural Language Toolkit --> Most Common\n",
        "\n",
        "import nltk  # Download key files\n",
        "nltk.download('punkt')  # Sequence Tokeniser\n",
        "nltk.download('stopwords')  # List of Most Common StopWords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string \n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import pad\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import RMSprop\n",
        "\n",
        "\n",
        "# Set Device to GPU is available - otherwise set to CPU: \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Your Current Device is {device}')  # Check the Colab Device we are using\n",
        "\n",
        "from torchtext import data, datasets  # Import the datasets\n",
        "from sklearn.model_selection import train_test_split  # Import splitting function\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torchdata\n",
        "\n",
        "from torchtext.vocab import GloVe  # Import the Glove Embedding"
      ],
      "metadata": {
        "id": "IdJSwr7e4-4-"
      },
      "id": "IdJSwr7e4-4-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the Train, Validation and Training Sets ready: "
      ],
      "metadata": {
        "id": "3StZWGLJ64ht"
      },
      "id": "3StZWGLJ64ht"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the train and test splits form the IMDB Dataset\n",
        "train_dataset, test_dataset  = datasets.IMDB(root = '.data', split = ('train', 'test'))\n",
        "\n",
        "# Let's now split the test set into a test and validation set: \n",
        "test_dataset, valid_dataset = train_test_split(list(test_dataset), train_size=.8)\n"
      ],
      "metadata": {
        "id": "J_uRE7P26-JJ"
      },
      "id": "J_uRE7P26-JJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset: \n",
        "IMDB Reviews\n"
      ],
      "metadata": {
        "id": "fXfE-9e56_QH"
      },
      "id": "fXfE-9e56_QH"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The shape of the training set is {train_dataset.shape}')"
      ],
      "metadata": {
        "id": "ObXgtrWG7Yrd"
      },
      "id": "ObXgtrWG7Yrd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of the training Dataset is XXX. \n",
        "\n",
        "Let's Check if our data is balanced in the training set: "
      ],
      "metadata": {
        "id": "PwZZaanj7sKH"
      },
      "id": "PwZZaanj7sKH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to check for balanced dataset"
      ],
      "metadata": {
        "id": "OSQ0AsiP7vO0"
      },
      "id": "OSQ0AsiP7vO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's now visualise some of the reviews: "
      ],
      "metadata": {
        "id": "0I7jEXXh7Vvk"
      },
      "id": "0I7jEXXh7Vvk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the First 2 Reviews\n",
        "train_dataset[:2]"
      ],
      "metadata": {
        "id": "0sTYA9Tg7ElX"
      },
      "id": "0sTYA9Tg7ElX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To summarise the dataset: \n",
        "- The dataset consists of Movie reviews taken from IMDB\n",
        "- The train set is formed of XXX reviews\n",
        "- The validation set of XXX reviews\n",
        "- and the Test set is of XXX reviews. \n",
        "- In the Y variable, a 1 consists of a Negative Review and 2 a Positive Review\n",
        "- We can also see that our dataset is BALANCED OR UNBALANCED"
      ],
      "metadata": {
        "id": "fTsu999e7Khe"
      },
      "id": "fTsu999e7Khe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing: \n",
        "\n",
        "For simlpe NLP applications, the data has to be processed in a certain manner: \n",
        "- Things to Check For:\n",
        "  - All lower-case text\n",
        "  - No Numbers in text\n",
        "  - No Punctutation - Good for generalisation - eventhough some people use punctuation to show sentiment\n",
        "- Transformation of the sentences into list of tokens - Therefore the sentence becomes a list of words\n",
        "- We have to tokenise the words - We will be using words from the GloVe library.\n",
        "  - We want to get the Index of our words in the GloVe library.\n",
        "- Padding of the sentences is also required as some of the reviews are very long or relatively short. Let's use a maximum padding of 150 here - For no reason."
      ],
      "metadata": {
        "id": "1mV1sWUG-cLb"
      },
      "id": "1mV1sWUG-cLb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Functions: \n",
        "- To Remove Numbers in the sentences\n",
        "- To Remove Punctuation\n",
        "- To Tokenise the sentences\n",
        "- To Remove Unwanted Stopwords\n",
        "- To Get the Index of the words in the GloVe library\n",
        "- To Pad the Sentences\n",
        "- A final Function which transforms the inputted test by using all the above functions and converting the sentences to lowercase."
      ],
      "metadata": {
        "id": "zUs4TgkkBQZM"
      },
      "id": "zUs4TgkkBQZM"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  '''Function to Remove Numbers from inputted text'''\n",
        "\n",
        "    text = ''.join(word for word in text if not word.isdigit())\n",
        "    return text\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  '''Function to Remove all Punctuation from inputted text'''\n",
        "\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, '')  # Replace the Punctuation with empty space\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "  '''Function to Tokenise any inputted text using NLTK tokenise'''\n",
        "\n",
        "    word_tokens = word_tokenize(text)  # Tokenise Using the NLTK Tokenise Function\n",
        "    return word_tokens\n",
        "\n",
        "def remove_stopwords(word_tokens, language='english'):\n",
        "  '''Function to remove all stopwords in given language from the inputted words tokens'''\n",
        "\n",
        "    stop_words = set(stopwords.words(language))  # Most common English Stopwords\n",
        "    word_tokens = [w for w in word_tokens if not w in stop_words]  # Get list of words if they are not stopwords\n",
        "    return word_tokens\n",
        "\n",
        "def get_index(text, vocab=glove):\n",
        "  '''Function that gets the index of each token in a text from the GloVe Library'''\n",
        "\n",
        "    embedded_text = []\n",
        "    for word in text:\n",
        "        try:\n",
        "            embedded_text.append(glove.stoi[word])  # Get String to Integer\n",
        "        except:\n",
        "            pass\n",
        "    return embedded_text  # return list of the indices of the tokenised words in the GloVe library\n",
        "\n",
        "\n",
        "def pad_sentence(text, MAX_LENGTH = 150):\n",
        "  ''' Function that Pads a sentence to a given length'''\n",
        "\n",
        "    if text.shape[0]>=MAX_LENGTH:\n",
        "        return text[:MAX_LENGTH]\n",
        "    else:\n",
        "        return pad(text, (0, MAX_LENGTH-text.shape[0]), 'constant',0).long()\n",
        "\n",
        "\n",
        "# Final Transform Function: \n",
        "\n",
        "def transform_text(text):\n",
        "  '''Function that applies all the Data-Preprocessing Functions'''\n",
        "  \n",
        "    text = text.lower()\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = tokenize(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = torch.tensor(get_index(text)).long()\n",
        "    return pad_sentence(text)"
      ],
      "metadata": {
        "id": "NwfFDYvXBNsd"
      },
      "id": "NwfFDYvXBNsd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have setup our data-preprocessing, let's test it out on an example from our training dataset: "
      ],
      "metadata": {
        "id": "FBDR7GjDFC7_"
      },
      "id": "FBDR7GjDFC7_"
    },
    {
      "cell_type": "code",
      "source": [
        "example_train = train_dataset[0][5]  # Get a random train data\n",
        "transform_text(example_train)"
      ],
      "metadata": {
        "id": "byexTK2OFJdI"
      },
      "id": "byexTK2OFJdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our pre-processing function seem to work as we want - Let's finalise the dataset with a dataloader. \n",
        "\n",
        "To not apply our transform function to all the data at once, let'd just apply it batch by batch from the dataloader using transform_batch function: "
      ],
      "metadata": {
        "id": "RC8smoypHgYp"
      },
      "id": "RC8smoypHgYp"
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    \n",
        "    X_embedded = torch.stack([transform_text(txt) for txt in X])  # Get the transformed - embedded text\n",
        "    \n",
        "    return X_embedded, torch.tensor(Y).long()-1  # Return the Embedded text and the Y variable as .long() as it is a categorical label\n",
        "\n",
        "train_dataset=  to_map_style_dataset(train_dataset)  # We will be using the to_map_style_dataset as it CHECK WHAT IT DOES\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=transform_batch, shuffle=True)  # Make sure to have shuffle on true for best training"
      ],
      "metadata": {
        "id": "4o6jcFJdHfar"
      },
      "id": "4o6jcFJdHfar",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}